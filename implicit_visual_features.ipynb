{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from data.utils import *\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from forecasting.lstm import LSTMModel, MotionPredictionDataset, k_best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 41\n",
    "VEHICLE_CLASSES = (\"car\", \"truck\", \"bus\", \"construction_vehicle\", \"emergency_vehicle\")\n",
    "FEATURE_SCALING = np.array([40, 40, 5, 5, 1])\n",
    "CENTER_FEATURE_SCALING = 4\n",
    "\n",
    "def delta_to_heading(delta):\n",
    "    return np.arctan2(delta[0], delta[1])\n",
    "\n",
    "def wrap_radians(radians):\n",
    "    \"\"\"\n",
    "    Wraps radians to the -pi, pi range.\n",
    "\n",
    "    Parameters:\n",
    "    radians (float): The radians to be wrapped.\n",
    "\n",
    "    Returns:\n",
    "    float: The wrapped radians.\n",
    "    \"\"\"\n",
    "    return (radians + np.pi) % (2 * np.pi) - np.pi\n",
    "\n",
    "def preprocess_data(labels, center_features=None, add_turn_label=False, max_len=20, prediction_len=6):\n",
    "    # group tracks\n",
    "    vehicle_labels = filter_by_class_names(labels, VEHICLE_CLASSES)\n",
    "    grouped_tracks = defaultdict(dict)\n",
    "    for seq_id, frames in vehicle_labels.items():\n",
    "        for timestep, frame in enumerate(frames):\n",
    "            for instance in array_dict_iterator(frame, len(frame[\"translation\"])):\n",
    "                grouped_tracks[f\"{seq_id}:{instance['track_id']}\"][timestep] = instance\n",
    "\n",
    "    data = {}\n",
    "    for id, instance_by_ts in progressbar(\n",
    "        grouped_tracks.items(), desc=\"preprocessing track data\"\n",
    "    ):\n",
    "        seq_id, instance_id = id.split(\":\")\n",
    "        class_label = next(iter(instance_by_ts.values()))[\"label\"]\n",
    "        features, targets, masks = [], [], []\n",
    "        # check ego distance\n",
    "        if np.any([\n",
    "            np.linalg.norm(instance[\"translation\"][:2] - np.array(instance[\"ego_translation\"])[:2]) >= 40\n",
    "            for instance in instance_by_ts.values()\n",
    "        ]):\n",
    "            continue\n",
    "\n",
    "        # change all sequences to start at timestep=0\n",
    "        min_ts = min(instance_by_ts.keys())\n",
    "        instance_by_ts = {\n",
    "            ts - min_ts: instance for ts, instance in instance_by_ts.items()\n",
    "        }\n",
    "        sorted_ts = list(sorted(instance_by_ts.keys()))\n",
    "        last_timestep = sorted_ts[-1]\n",
    "\n",
    "        # linearly interpolate missing translations\n",
    "        translations = {}\n",
    "        for timestep in range(last_timestep+1):\n",
    "            if timestep in instance_by_ts:\n",
    "                translations[timestep] = instance_by_ts[timestep][\"translation\"][:2]\n",
    "            else:\n",
    "                prev_timestep = next(ts for ts in reversed(sorted_ts) if ts <= timestep)\n",
    "                next_timestep = next(ts for ts in sorted_ts if ts >= timestep)\n",
    "                translation = (\n",
    "                    instance_by_ts[next_timestep][\"translation\"][:2] * (timestep - prev_timestep)\n",
    "                    + instance_by_ts[prev_timestep][\"translation\"][:2] * (next_timestep - timestep)\n",
    "                ) / (next_timestep - prev_timestep)\n",
    "                translations[timestep] = translation\n",
    "\n",
    "        for timestep in range(max_len):\n",
    "            # if features are missing, default to zeros\n",
    "            feature_size = 5\n",
    "            if add_turn_label:\n",
    "                feature_size += 1\n",
    "            if center_features is not None:\n",
    "                feature_size += len(next(iter(center_features.values())))\n",
    "            feature = np.zeros(feature_size)\n",
    "            target = np.zeros((prediction_len, 2))\n",
    "            mask = np.zeros(prediction_len)\n",
    "            if timestep in instance_by_ts:\n",
    "                instance = instance_by_ts[timestep]\n",
    "                # format features (translation, velocity)\n",
    "                translation_delta = (\n",
    "                    instance[\"translation\"] - instance_by_ts[0][\"translation\"]\n",
    "                )\n",
    "                # normalize inputs\n",
    "                feature = np.nan_to_num(\n",
    "                    np.concatenate(\n",
    "                        [\n",
    "                            translation_delta[:2],\n",
    "                            instance[\"velocity\"][:2],\n",
    "                            np.array([instance[\"yaw\"]]),\n",
    "                        ]\n",
    "                    )\n",
    "                    / FEATURE_SCALING\n",
    "                )\n",
    "                # transform target to translations\n",
    "                future_timesteps = list(range(\n",
    "                    timestep + 1, min(timestep + prediction_len + 1, last_timestep + 1)\n",
    "                ))\n",
    "                for i, future_ts in enumerate(future_timesteps):\n",
    "                    # target[i] = translations[future_ts] - instance[\"translation\"][:2]\n",
    "                    target[i] = translations[future_ts] - translations[future_ts - 1]\n",
    "                    mask[i] = 1\n",
    "\n",
    "                if center_features is not None:\n",
    "                    center_feature = center_features[f\"{seq_id}:{instance['timestamp_ns']}:{instance_id}\"] / CENTER_FEATURE_SCALING\n",
    "                    feature = np.concatenate([feature, center_feature])\n",
    "\n",
    "                if add_turn_label:\n",
    "                    future_timesteps = [ts for ts in future_timesteps if ts in instance_by_ts]\n",
    "                    if len(future_timesteps) == 0:\n",
    "                        turn_radian = 0\n",
    "                    else:\n",
    "                        final_delta = instance_by_ts[future_timesteps[-1]][\"translation\"] - instance[\"translation\"]\n",
    "                        turn_radian = np.nan_to_num(\n",
    "                            wrap_radians(delta_to_heading(final_delta) - instance[\"yaw\"])\n",
    "                        ) if np.linalg.norm(final_delta) > 2 else 0\n",
    "                    turn_indicator = 0\n",
    "                    if 0.1 * np.pi <= turn_radian:\n",
    "                        turn_indicator = 1\n",
    "                    if turn_radian <= -0.1 * np.pi:\n",
    "                        turn_indicator = -1\n",
    "                    feature = np.concatenate([feature, np.array([turn_indicator])])\n",
    "\n",
    "            features.append(feature)\n",
    "            targets.append(target)\n",
    "            masks.append(mask)\n",
    "\n",
    "        data[id] = {\n",
    "            \"feature\": np.stack(features, axis=0),\n",
    "            \"target\": np.stack(targets, axis=0),\n",
    "            \"target_mask\": np.stack(masks, axis=0),\n",
    "            \"class_label\": class_label,\n",
    "            \"min_ts\": min_ts,\n",
    "        }\n",
    "\n",
    "    return data\n",
    "\n",
    "class CenterFeatureModel(LSTMModel):\n",
    "    def __init__(self, input_dim, prediction_len, k, **kwargs):\n",
    "        super().__init__(input_dim - 128, prediction_len, k, **kwargs)\n",
    "        self.center_feature_proj = nn.Linear(128, self.embedding_dim)\n",
    "        nn.init.kaiming_uniform_(self.center_feature_proj.weight)\n",
    "\n",
    "    def forward(self, input):\n",
    "        center_features = input[:, :, -128:]\n",
    "        input = input[:, :, :-128]\n",
    "        B, L, D = input.shape\n",
    "        embedding = (\n",
    "            self.input_proj(input.reshape(B * L, -1)).reshape(B, L, -1) +\n",
    "            self.center_feature_proj(center_features.reshape(B * L, -1)).reshape(B, L, -1)\n",
    "        )\n",
    "        x = F.relu(embedding)\n",
    "        for lstm_layer in self.lstm_layers:\n",
    "            x_out, state = lstm_layer(x)\n",
    "            x = x + self.dropout(x_out)\n",
    "        output = self.output_proj(x.reshape(B * L, -1)).reshape(\n",
    "            B, L, self.k, self.prediction_len, -1\n",
    "        )\n",
    "        return output\n",
    "\n",
    "shared_values = lambda d1, d2: ((d1[key], d2[key]) for key in set(d1.keys()).intersection(d2.keys()))\n",
    "\n",
    "def run_and_evaluate_inference(model, dataset):\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    model = model.to(device).eval()\n",
    "    forecasts = defaultdict(lambda: defaultdict(dict))\n",
    "    start_idx = 0\n",
    "    for input, _, target, mask in dataloader:\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        with torch.no_grad():\n",
    "            prediction = model(input)\n",
    "        prediction = prediction.cpu().numpy()\n",
    "        ids = dataset.keys[start_idx:start_idx + len(input)]\n",
    "        start_idx += len(input)\n",
    "        for id, prediction_ot in zip(ids, prediction):\n",
    "            start_ts = dataset.data[id][\"min_ts\"]\n",
    "            seq_id, track_id = id.split(\":\")\n",
    "            for i, delta_at_t in enumerate(prediction_ot):\n",
    "                timestep = start_ts + i\n",
    "                if timestep >= len(labels[seq_id]):\n",
    "                    continue\n",
    "                detection_frame = labels[seq_id][timestep]\n",
    "                if track_id not in detection_frame[\"track_id\"]:\n",
    "                    continue\n",
    "                timestamp = detection_frame[\"timestamp_ns\"]\n",
    "                detection = index_array_values(detection_frame, list(detection_frame[\"track_id\"]).index(track_id))\n",
    "                current_translation = detection[\"translation\"][:2]\n",
    "                prediction_at_t = current_translation + np.cumsum(delta_at_t, axis=1)\n",
    "                forecasts[seq_id][timestamp][track_id] = prediction_at_t\n",
    "    forecasts = {seq_id: dict(preds_by_ts) for seq_id, preds_by_ts in forecasts.items()}\n",
    "\n",
    "    errors_by_profile = {p: {\"ade\": [], \"fde\": []} for p in velocity_profile}\n",
    "    for forecast_frames, forecast_label_frames in shared_values(forecasts, forecast_label):\n",
    "        for prediction_by_instance_id, label_list in shared_values(forecast_frames, forecast_label_frames):\n",
    "            label_by_instance_id = {label_agent[\"instance_id\"]: label_agent for label_agent in label_list}\n",
    "            for prediction, label_agent in shared_values(prediction_by_instance_id, label_by_instance_id):\n",
    "                profile = label_agent[\"trajectory_type\"]\n",
    "                label_length = label_agent[\"future_translation\"].shape[0]\n",
    "                errors = np.linalg.norm(prediction[:, :label_length] - label_agent[\"future_translation\"][np.newaxis], axis=-1)\n",
    "                ade = errors.mean(axis=1).min(axis=0)\n",
    "                errors_by_profile[profile][\"ade\"].append(ade)\n",
    "                if label_length == PREDICTION_LENGTH:\n",
    "                    fde = errors[:, -1].min(axis=0)\n",
    "                    errors_by_profile[profile][\"fde\"].append(fde)\n",
    "\n",
    "    metric_by_profile = {\n",
    "        p: {\n",
    "                \"ade\": np.mean(errors_by_profile[p][\"ade\"]),\n",
    "                \"fde\": np.mean(errors_by_profile[p][\"fde\"]),\n",
    "                \"count\": len(errors_by_profile[p][\"ade\"]),\n",
    "            }\n",
    "        for p in velocity_profile\n",
    "    }\n",
    "    return forecasts, metric_by_profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_LENGTH = 6\n",
    "K = 5\n",
    "\n",
    "train_labels = load(f\"dataset/nuscenes-train/labels.pkl\")\n",
    "labels = load(f\"dataset/nuscenes-val/labels.pkl\")\n",
    "center_features = load(\"dataset/center_features.pkl\")\n",
    "\n",
    "# evaluation\n",
    "from forecasting.evaluate import convert_forecast_labels, trajectory_type, av2_velocity, velocity_profile\n",
    "forecast_label = convert_forecast_labels(labels, PREDICTION_LENGTH, np.inf)\n",
    "for frames in forecast_label.values():\n",
    "    for frame in frames.values():\n",
    "        for agent in frame:\n",
    "            agent[\"trajectory_type\"] = trajectory_type(agent, av2_velocity, PREDICTION_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN oracle turn signals experiment\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "learning_rate = 1e-3\n",
    "device = \"cuda\"\n",
    "sweep_results = {}\n",
    "\n",
    "use_center_features, add_turn_label = False, True\n",
    "K = 5\n",
    "train_data = preprocess_data(\n",
    "    train_labels, max_len=MAX_LEN, prediction_len=PREDICTION_LENGTH,\n",
    "    add_turn_label=add_turn_label, center_features=(center_features if use_center_features else None),\n",
    "    )\n",
    "train_dataset = MotionPredictionDataset(train_data, PREDICTION_LENGTH)\n",
    "data = preprocess_data(\n",
    "    labels, max_len=MAX_LEN, prediction_len=PREDICTION_LENGTH,\n",
    "    add_turn_label=add_turn_label, center_features=(center_features if use_center_features else None),\n",
    ")\n",
    "dataset = MotionPredictionDataset(data, PREDICTION_LENGTH)\n",
    "dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    ")\n",
    "if use_center_features:\n",
    "    model = CenterFeatureModel(train_dataset.input_dim, train_dataset.prediction_len, k=K)\n",
    "else:\n",
    "    model = LSTMModel(train_dataset.input_dim, train_dataset.prediction_len, k=K)\n",
    "\n",
    "# run\n",
    "model = model.to(device).train()\n",
    "optim = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "loss_traj = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    epoch_loss = []\n",
    "    for input, _, target, mask in dataloader:\n",
    "        input, target, mask = input.to(device), target.to(device), mask.to(device)\n",
    "        prediction = model(input)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss = k_best_loss(prediction, target, mask)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        epoch_loss.append(loss.detach().cpu().item())\n",
    "    loss_traj.append(np.mean(epoch_loss))\n",
    "\n",
    "# torch.save(model, f\"models/model_dt_{K}{'_cf' if use_center_features else ''}{'_tl' if add_turn_label else ''}.pt\")\n",
    "# val\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    ")\n",
    "model = model.to(device).eval()\n",
    "\n",
    "val_loss = []\n",
    "for input, _, target, mask in dataloader:\n",
    "    input, target, mask = input.to(device), target.to(device), mask.to(device)\n",
    "    with torch.no_grad():\n",
    "        prediction = model(input)\n",
    "        val_loss.append(k_best_loss(prediction, target, mask).cpu().item())\n",
    "\n",
    "_, metric_by_profile = run_and_evaluate_inference(model, dataset)\n",
    "print(metric_by_profile)\n",
    "experiment_results = {\"metrics\": metric_by_profile, \"validation_loss\": np.mean(val_loss), \"train_loss\": loss_traj}\n",
    "sweep_results[(K, use_center_features, add_turn_label)] = experiment_results\n",
    "# save(sweep_results, \"sweep_results_dt.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN K experiment\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "learning_rate = 1e-3\n",
    "device = \"cuda\"\n",
    "sweep_results = {}\n",
    "\n",
    "for use_center_features, add_turn_label in [(False, False), (True, False)]:\n",
    "    train_data = preprocess_data(\n",
    "        train_labels, max_len=MAX_LEN, prediction_len=PREDICTION_LENGTH,\n",
    "        add_turn_label=add_turn_label, center_features=(center_features if use_center_features else None),\n",
    "        )\n",
    "    train_dataset = MotionPredictionDataset(train_data, PREDICTION_LENGTH)\n",
    "    data = preprocess_data(\n",
    "        labels, max_len=MAX_LEN, prediction_len=PREDICTION_LENGTH,\n",
    "        add_turn_label=add_turn_label, center_features=(center_features if use_center_features else None),\n",
    "    )\n",
    "    dataset = MotionPredictionDataset(data, PREDICTION_LENGTH)\n",
    "    dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    for K in tqdm([1, 3, 5], desc=\"sweeping K\"):\n",
    "        if use_center_features:\n",
    "            model = CenterFeatureModel(train_dataset.input_dim, train_dataset.prediction_len, k=K)\n",
    "        else:\n",
    "            model = LSTMModel(train_dataset.input_dim, train_dataset.prediction_len, k=K)\n",
    "\n",
    "        # run\n",
    "        model = model.to(device).train()\n",
    "        optim = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "        loss_traj = []\n",
    "\n",
    "        for epoch in range(50):\n",
    "            epoch_loss = []\n",
    "            for input, _, target, mask in dataloader:\n",
    "                input, target, mask = input.to(device), target.to(device), mask.to(device)\n",
    "                prediction = model(input)\n",
    "\n",
    "                optim.zero_grad()\n",
    "                loss = k_best_loss(prediction, target, mask)\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                epoch_loss.append(loss.detach().cpu().item())\n",
    "            loss_traj.append(np.mean(epoch_loss))\n",
    "\n",
    "        # torch.save(model, f\"models/model_dt_{K}{'_cf' if use_center_features else ''}{'_tl' if add_turn_label else ''}.pt\")\n",
    "        # val\n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        model = model.to(device).eval()\n",
    "\n",
    "        val_loss = []\n",
    "        for input, _, target, mask in dataloader:\n",
    "            input, target, mask = input.to(device), target.to(device), mask.to(device)\n",
    "            with torch.no_grad():\n",
    "                prediction = model(input)\n",
    "                val_loss.append(k_best_loss(prediction, target, mask).cpu().item())\n",
    "\n",
    "        _, metric_by_profile = run_and_evaluate_inference(model, dataset)\n",
    "        print(metric_by_profile)\n",
    "        experiment_results = {\"metrics\": metric_by_profile, \"validation_loss\": np.mean(val_loss), \"train_loss\": loss_traj}\n",
    "        sweep_results[(K, use_center_features, add_turn_label)] = experiment_results\n",
    "# save(sweep_results, \"sweep_results_dt.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sweep_results[(K, False, False)][\"metrics\"])\n",
    "print(sweep_results[(K, True, False)][\"metrics\"])\n",
    "print(sweep_results[(K, False, True)][\"metrics\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "use_center_features = False\n",
    "add_turn_label = False\n",
    "train_data = preprocess_data(\n",
    "    train_labels, max_len=MAX_LEN, prediction_len=PREDICTION_LENGTH,\n",
    "    add_turn_label=add_turn_label, center_features=(center_features if use_center_features else None),\n",
    "    )\n",
    "train_dataset = MotionPredictionDataset(train_data, PREDICTION_LENGTH)\n",
    "data = preprocess_data(\n",
    "    labels, max_len=MAX_LEN, prediction_len=PREDICTION_LENGTH,\n",
    "    add_turn_label=add_turn_label, center_features=(center_features if use_center_features else None),\n",
    ")\n",
    "dataset = MotionPredictionDataset(data, PREDICTION_LENGTH)\n",
    "\n",
    "# model = torch.load(f\"models/model_dt_5{'_cf' if use_center_features else ''}{'_tl' if add_turn_label else ''}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    ")\n",
    "if use_center_features:\n",
    "    model = CenterFeatureModel(train_dataset.input_dim, train_dataset.prediction_len, k=K)\n",
    "else:\n",
    "    model = LSTMModel(train_dataset.input_dim, train_dataset.prediction_len, k=K)\n",
    "\n",
    "# run\n",
    "learning_rate = 1e-3\n",
    "device = \"cuda\"\n",
    "model = model.to(device).train()\n",
    "optim = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "loss_traj = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    epoch_loss = []\n",
    "    for input, _, target, mask in dataloader:\n",
    "        input, target, mask = input.to(device), target.to(device), mask.to(device)\n",
    "        prediction = model(input)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss = k_best_loss(prediction, target, mask)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        epoch_loss.append(loss.detach().cpu().item())\n",
    "\n",
    "    # print(epoch, np.mean(epoch_loss))\n",
    "    loss_traj.append(np.mean(epoch_loss))\n",
    "\n",
    "# val\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    ")\n",
    "model = model.to(device).eval()\n",
    "\n",
    "val_loss = []\n",
    "for input, _, target, mask in dataloader:\n",
    "    input, target, mask = input.to(device), target.to(device), mask.to(device)\n",
    "    with torch.no_grad():\n",
    "        prediction = model(input)\n",
    "        val_loss.append(k_best_loss(prediction, target, mask).cpu().item())\n",
    "print(\"validation loss:\", np.mean(val_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, f\"models/model{'_cf' if use_center_features else ''}{'_tl' if add_turn_label else ''}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "\n",
    "import json\n",
    "_, metric_by_profile = run_and_evaluate_inference(model ,dataset)\n",
    "print(json.dumps(metric_by_profile, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes import NuScenes\n",
    "\n",
    "data_root = '/data/ashen3/datasets/nuScenes'\n",
    "nusc = NuScenes(\"v1.0-trainval\", data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_turn_label = False\n",
    "use_center_features = False\n",
    "data = preprocess_data(\n",
    "    labels, max_len=MAX_LEN, prediction_len=PREDICTION_LENGTH,\n",
    "    add_turn_label=add_turn_label, center_features=(center_features if use_center_features else None),\n",
    ")\n",
    "dataset = MotionPredictionDataset(data, PREDICTION_LENGTH)\n",
    "model = torch.load(f\"models/model_dt_5{'_cf' if use_center_features else ''}{'_tl' if add_turn_label else ''}.pt\")\n",
    "baseline_forecasts, _ = run_and_evaluate_inference(model, dataset)\n",
    "\n",
    "use_center_features = True\n",
    "data = preprocess_data(\n",
    "    labels, max_len=MAX_LEN, prediction_len=PREDICTION_LENGTH,\n",
    "    add_turn_label=add_turn_label, center_features=(center_features if use_center_features else None),\n",
    ")\n",
    "dataset = MotionPredictionDataset(data, PREDICTION_LENGTH)\n",
    "model = torch.load(f\"models/model_dt_5{'_cf' if use_center_features else ''}{'_tl' if add_turn_label else ''}.pt\")\n",
    "cf_forecasts, _ = run_and_evaluate_inference(model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from nuscenes.map_expansion.map_api import NuScenesMap\n",
    "\n",
    "def get_nusc_map(nusc, scene_token):\n",
    "    scene_rec = nusc.get('scene', scene_token)\n",
    "    log_record = nusc.get('log', scene_rec['log_token'])\n",
    "    map_name = log_record['location']\n",
    "    nusc_map = NuScenesMap(dataroot=data_root, map_name=map_name)\n",
    "    return nusc_map\n",
    "\n",
    "def plot_forecasts(forecasts, seq_id, timestamp, margin=10):\n",
    "    prediction_by_instance_id = forecasts[seq_id][timestamp]\n",
    "    label_by_instance_id = {agent[\"instance_id\"]: agent for agent in forecast_label[seq_id][timestamp]}\n",
    "    locations = []\n",
    "    for prediction, label_agent in shared_values(prediction_by_instance_id, label_by_instance_id):\n",
    "        if label_agent[\"trajectory_type\"] != \"static\":\n",
    "            locations.append(prediction)\n",
    "    locations = np.stack(locations, axis=0).reshape(-1, 2)\n",
    "    m = margin\n",
    "    box_coords = locations[:, 0].min() - m, locations[:, 1].min() - m, locations[:, 0].max() + m, locations[:, 1].max() + m\n",
    "    layer_names = [\n",
    "    'drivable_area',\n",
    "    'road_segment',\n",
    "    'road_block',\n",
    "    'lane',\n",
    "    'stop_line',\n",
    "    'road_divider',\n",
    "    'lane_divider',\n",
    "    ]\n",
    "    nusc_map.render_map_patch(box_coords, layer_names=layer_names, figsize=(6, 6))\n",
    "\n",
    "    for prediction, label_agent in shared_values(prediction_by_instance_id, label_by_instance_id):\n",
    "        loc = label_agent[\"current_translation\"]\n",
    "        plt.scatter(*loc, c=\"black\")\n",
    "        # skip static agents\n",
    "        if label_agent[\"trajectory_type\"] == \"static\":\n",
    "            continue\n",
    "        gt_future = label_agent[\"future_translation\"]\n",
    "        # only plot best prediction\n",
    "        best_pred_i = np.argmin(np.linalg.norm(prediction[:, :len(gt_future)] - gt_future[np.newaxis], axis=-1).mean(1))\n",
    "        prd_trajectory = np.concatenate([loc[np.newaxis], prediction[best_pred_i]])\n",
    "        plt.plot(prd_trajectory[:, 0], prd_trajectory[:, 1], c=\"blue\")\n",
    "        # plot all predictions\n",
    "        for predicted_mode in prediction:\n",
    "            prd_trajectory = np.concatenate([loc[np.newaxis], predicted_mode])\n",
    "            plt.plot(prd_trajectory[:, 0], prd_trajectory[:, 1], c=\"blue\")\n",
    "        # plot ground truth\n",
    "        gt_trajectory = np.concatenate([loc[np.newaxis], gt_future])\n",
    "        plt.plot(gt_trajectory[:, 0], gt_trajectory[:, 1], c=\"black\")\n",
    "\n",
    "    plt.xlim(box_coords[0], box_coords[2])\n",
    "    plt.ylim(box_coords[1], box_coords[3])\n",
    "    plt.show()\n",
    "\n",
    "# visualize predictions\n",
    "seq_id = 'ed242d80ccb34b139aaf9ab89859332e'\n",
    "timestamp = 1535730476396726000\n",
    "nusc_map = get_nusc_map(nusc, seq_id)\n",
    "\n",
    "plot_forecasts(cf_forecasts, seq_id, timestamp)\n",
    "plt.figure()\n",
    "plot_forecasts(baseline_forecasts, seq_id, timestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "plot_forecasts(cf_forecasts, seq_id, timestamp, margin=20)\n",
    "# plt.xlim(1415, 1440)\n",
    "# plt.ylim(1220, 1255)\n",
    "plt.xlim(1430, 1465)\n",
    "plt.ylim(1225, 1265)\n",
    "plt.savefig(f\"plots/visual_features_neg_{seq_id}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "plot_forecasts(baseline_forecasts, seq_id, timestamp, margin=20)\n",
    "# plt.xlim(1415, 1440)\n",
    "# plt.ylim(1220, 1255)\n",
    "plt.xlim(1430, 1465)\n",
    "plt.ylim(1225, 1265)\n",
    "plt.savefig(f\"plots/baseline_neg_{seq_id}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_and_count = []\n",
    "shared_items = lambda d1, d2: ((key, (d1[key], d2[key])) for key in set(d1.keys()).intersection(d2.keys()))\n",
    "for seq_id, (forecast_frames, forecast_label_frames) in shared_items(forecasts, forecast_label):\n",
    "    for timestamp, (prediction_by_instance_id, label_list) in shared_items(forecast_frames, forecast_label_frames):\n",
    "        label_by_instance_id = {label_agent[\"instance_id\"]: label_agent for label_agent in label_list}\n",
    "        num_non_static = 0\n",
    "        for prediction, label_agent in shared_values(prediction_by_instance_id, label_by_instance_id):\n",
    "            profile = label_agent[\"trajectory_type\"]\n",
    "            num_non_static += profile != \"static\"\n",
    "        frames_and_count.append((num_non_static, seq_id, timestamp))\n",
    "\n",
    "sorted(frames_and_count, reverse=True)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmdet3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
